{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c6e0b4",
   "metadata": {
    "papermill": {
     "duration": 0.003186,
     "end_time": "2025-08-18T20:38:20.023458",
     "exception": false,
     "start_time": "2025-08-18T20:38:20.020272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest → AWS S3 (BLS/DataUSA)\n",
    "This notebook retrieves data from the BLS and DataUSA APIs and stores it in Amazon S3.  \n",
    "It demonstrates the first step in building a data pipeline — collecting raw data and landing it in cloud storage.\n",
    "\n",
    "**[View Enhanced Sync Version](https://github.com/ScottySchmidt/AWS_DataEngineer_API/blob/main/01-ingest-api-sync.ipynb)**\n",
    "\n",
    "### What's Covered\n",
    "- **Simple ingestion** from the BLS and DataUSA APIs.\n",
    "- **Secure key management** with Kaggle secrets (AWS + API keys).\n",
    "- **JSON outputs in S3** organized by source and date.\n",
    "- **Reusable structure** for later pipeline steps (hashing, dedup, reporting)..\n",
    "\n",
    "### How It Works\n",
    "1. Call the BLS and DataUSA APIs.\n",
    "2. Save the raw JSON responses locally in memory.\n",
    "3. Upload each response to the configured S3 bucket.\n",
    "4. Organize files by source and date for later processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cee7e2",
   "metadata": {
    "papermill": {
     "duration": 0.002409,
     "end_time": "2025-08-18T20:38:20.028863",
     "exception": false,
     "start_time": "2025-08-18T20:38:20.026454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to AWS S3\n",
    "This notebook requires the following Python packages:  \n",
    "- boto3  \n",
    "- requests  \n",
    "- hashlib  \n",
    "- kaggle_secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0dcca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:38:20.035991Z",
     "iopub.status.busy": "2025-08-18T20:38:20.035666Z",
     "iopub.status.idle": "2025-08-18T20:38:21.637009Z",
     "shell.execute_reply": "2025-08-18T20:38:21.635927Z"
    },
    "papermill": {
     "duration": 1.606984,
     "end_time": "2025-08-18T20:38:21.638790",
     "exception": false,
     "start_time": "2025-08-18T20:38:20.031806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  40\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebf6b8",
   "metadata": {
    "papermill": {
     "duration": 0.002503,
     "end_time": "2025-08-18T20:38:21.644567",
     "exception": false,
     "start_time": "2025-08-18T20:38:21.642064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch BLS Data via API Key\n",
    "- Authenticate with a registered BLS API key to comply with access policies.  \n",
    "- Retrieve U.S. inflation data programmatically through the BLS Public API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15eb769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:38:21.650941Z",
     "iopub.status.busy": "2025-08-18T20:38:21.650543Z",
     "iopub.status.idle": "2025-08-18T20:38:22.082647Z",
     "shell.execute_reply": "2025-08-18T20:38:22.081457Z"
    },
    "papermill": {
     "duration": 0.437556,
     "end_time": "2025-08-18T20:38:22.084806",
     "exception": false,
     "start_time": "2025-08-18T20:38:21.647250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded bls_data.json to S3.\n"
     ]
    }
   ],
   "source": [
    "# API payload\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\n",
    "    \"seriesid\": [\"CUUR0000SA0\", \"SUUR0000SA0\"],  # You can customize this\n",
    "    \"startyear\": \"2020\",\n",
    "    \"endyear\": \"2024\",\n",
    "    \"registrationkey\": API_KEY\n",
    "})\n",
    "\n",
    "# Make request\n",
    "response = requests.post(\n",
    "    \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    \n",
    "    # Save locally\n",
    "    with open(\"bls_data.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Upload JSON file to S3 bucket\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=\"bls_data.json\",\n",
    "        Body=json.dumps(results, indent=2)\n",
    "    )\n",
    "    \n",
    "    print(\"Uploaded bls_data.json to S3.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bac73a",
   "metadata": {
    "papermill": {
     "duration": 0.003916,
     "end_time": "2025-08-18T20:38:22.093067",
     "exception": false,
     "start_time": "2025-08-18T20:38:22.089151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **API Data Pipeline – Send Files to S3 (No API Key Needed)**\n",
    "- **Custom User-Agent** to comply with BLS access rules and avoid 403 errors.  \n",
    "- **Checks for changes** by comparing file hashes before uploading.  \n",
    "- **Uploads only when updated**, reducing bandwidth usage and S3 storage costs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f80e307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:38:22.105315Z",
     "iopub.status.busy": "2025-08-18T20:38:22.104802Z",
     "iopub.status.idle": "2025-08-18T20:38:22.583124Z",
     "shell.execute_reply": "2025-08-18T20:38:22.582127Z"
    },
    "papermill": {
     "duration": 0.486471,
     "end_time": "2025-08-18T20:38:22.584983",
     "exception": false,
     "start_time": "2025-08-18T20:38:22.098512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr.data.0.Current  File Downloaded from BLS\n",
      "No changes detected. Upload skipped.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "\n",
    "# CONFIG \n",
    "file_name = \"pr.data.0.Current\"\n",
    "bls_url = f\"https://download.bls.gov/pub/time.series/pr/{file_name}\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"ScottSchmidt/1.0 (scott.schmidt1989@yahoo.com)\"\n",
    "}\n",
    "\n",
    "# Download file from BLS:\n",
    "print(file_name, \" File Downloaded from BLS\")\n",
    "response = requests.get(bls_url, headers=headers)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to download BLS data:\", response.status_code)\n",
    "else:\n",
    "    content = response.text\n",
    "    content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    try:\n",
    "        existing_obj = s3.get_object(Bucket=BUCKET_NAME, Key=file_name)\n",
    "        existing_content = existing_obj['Body'].read().decode(\"utf-8\")\n",
    "        existing_hash = hashlib.md5(existing_content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if content_hash == existing_hash:\n",
    "            print(\"No changes detected. Upload skipped.\")\n",
    "        else:\n",
    "            s3.put_object(Bucket=BUCKET_NAME, Key=file_name, Body=content)\n",
    "            print(\"Updated file uploaded to S3.\")\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        # File doesn't exist yet\n",
    "        s3.put_object(Bucket=BUCKET_NAME, Key=file_name, Body=content)\n",
    "        print(\"File not found in S3. Uploaded new file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa9c30f",
   "metadata": {
    "papermill": {
     "duration": 0.002399,
     "end_time": "2025-08-18T20:38:22.590342",
     "exception": false,
     "start_time": "2025-08-18T20:38:22.587943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch and Upload BLS Data for Each Series ID\n",
    "This section goes through each BLS series ID, requests data from the API, \n",
    "and uploads it to S3 only if the data has changed since the last upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5c8ff87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:38:22.597449Z",
     "iopub.status.busy": "2025-08-18T20:38:22.597133Z",
     "iopub.status.idle": "2025-08-18T20:38:23.393597Z",
     "shell.execute_reply": "2025-08-18T20:38:23.392678Z"
    },
    "papermill": {
     "duration": 0.801835,
     "end_time": "2025-08-18T20:38:23.395135",
     "exception": false,
     "start_time": "2025-08-18T20:38:22.593300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching CUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  CUUR0000SA0.json\n",
      "Fetching SUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  SUUR0000SA0.json\n"
     ]
    }
   ],
   "source": [
    "# Series IDs you want to fetch\n",
    "series_ids = [\"CUUR0000SA0\", \"SUUR0000SA0\"]  \n",
    "\n",
    "# Loop through each series and fetch data from the BLS API\n",
    "for series_id in series_ids:\n",
    "    print(f\"Fetching {series_id} from BLS API...\")\n",
    "\n",
    "    # Prepare API request payload\n",
    "    payload = json.dumps({\n",
    "        \"seriesid\": [series_id],\n",
    "        \"startyear\": \"2020\",\n",
    "        \"endyear\": \"2024\",\n",
    "        \"registrationkey\": API_KEY\n",
    "    })\n",
    "\n",
    "    # Send POST request to the BLS API\n",
    "    response = requests.post(\n",
    "        \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "        data=payload,\n",
    "        headers={\"Content-type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    #  Skip if API request fails\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {series_id}: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    # Skip if API request fails\n",
    "    content = response.text\n",
    "    hash_new = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "    s3_key = f\"{series_id}.json\"\n",
    "\n",
    "    # Check if file already exists in S3 with same content\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "        existing = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        hash_existing = hashlib.md5(existing.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if hash_existing == hash_new:\n",
    "            print(series_id, \" Skipping. Unchanged series_id\")\n",
    "            continue\n",
    "    except ClientError:\n",
    "        pass  # File doesn't exist \n",
    "\n",
    "    # Upload to S3\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=content)\n",
    "    print(\"Uploaded to S3! \", s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31742c",
   "metadata": {
    "papermill": {
     "duration": 0.00255,
     "end_time": "2025-08-18T20:38:23.400510",
     "exception": false,
     "start_time": "2025-08-18T20:38:23.397960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview BLS Data from S3\n",
    "This section retrieves a specific BLS JSON file from Amazon S3, \n",
    "converts it into a Pandas DataFrame, reorders the columns, \n",
    "and displays the first few rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a5d7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T20:38:23.407071Z",
     "iopub.status.busy": "2025-08-18T20:38:23.406758Z",
     "iopub.status.idle": "2025-08-18T20:38:25.484597Z",
     "shell.execute_reply": "2025-08-18T20:38:25.483576Z"
    },
    "papermill": {
     "duration": 2.082909,
     "end_time": "2025-08-18T20:38:25.486146",
     "exception": false,
     "start_time": "2025-08-18T20:38:23.403237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  (60, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>M12</td>\n",
       "      <td>315.605</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>M11</td>\n",
       "      <td>315.493</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>M10</td>\n",
       "      <td>315.664</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>M09</td>\n",
       "      <td>315.301</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>M08</td>\n",
       "      <td>314.796</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>M07</td>\n",
       "      <td>314.540</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>M06</td>\n",
       "      <td>314.175</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>M05</td>\n",
       "      <td>314.069</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>M04</td>\n",
       "      <td>313.548</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>M03</td>\n",
       "      <td>312.332</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024</td>\n",
       "      <td>M02</td>\n",
       "      <td>310.326</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024</td>\n",
       "      <td>M01</td>\n",
       "      <td>308.417</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023</td>\n",
       "      <td>M12</td>\n",
       "      <td>306.746</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023</td>\n",
       "      <td>M11</td>\n",
       "      <td>307.051</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023</td>\n",
       "      <td>M10</td>\n",
       "      <td>307.671</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023</td>\n",
       "      <td>M09</td>\n",
       "      <td>307.789</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023</td>\n",
       "      <td>M08</td>\n",
       "      <td>307.026</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>M07</td>\n",
       "      <td>305.691</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023</td>\n",
       "      <td>M06</td>\n",
       "      <td>305.109</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023</td>\n",
       "      <td>M05</td>\n",
       "      <td>304.127</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year period    value footnotes\n",
       "0   2024    M12  315.605      [{}]\n",
       "1   2024    M11  315.493      [{}]\n",
       "2   2024    M10  315.664      [{}]\n",
       "3   2024    M09  315.301      [{}]\n",
       "4   2024    M08  314.796      [{}]\n",
       "5   2024    M07  314.540      [{}]\n",
       "6   2024    M06  314.175      [{}]\n",
       "7   2024    M05  314.069      [{}]\n",
       "8   2024    M04  313.548      [{}]\n",
       "9   2024    M03  312.332      [{}]\n",
       "10  2024    M02  310.326      [{}]\n",
       "11  2024    M01  308.417      [{}]\n",
       "12  2023    M12  306.746      [{}]\n",
       "13  2023    M11  307.051      [{}]\n",
       "14  2023    M10  307.671      [{}]\n",
       "15  2023    M09  307.789      [{}]\n",
       "16  2023    M08  307.026      [{}]\n",
       "17  2023    M07  305.691      [{}]\n",
       "18  2023    M06  305.109      [{}]\n",
       "19  2023    M05  304.127      [{}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file content \n",
    "key = \"CUUR0000SA0.json\" # SUUR0000SA0\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "json_content = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract data into DataFrame\n",
    "series_data = json_content['Results']['series'][0]['data']\n",
    "df = pd.DataFrame(series_data)\n",
    "df = df[[\"year\", \"period\", \"value\", \"footnotes\"]]\n",
    "print(\"DataFrame Shape: \", df.shape)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.032949,
   "end_time": "2025-08-18T20:38:26.109786",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T20:38:15.076837",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
