{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb048a2",
   "metadata": {
    "papermill": {
     "duration": 0.003044,
     "end_time": "2025-08-18T22:10:02.164439",
     "exception": false,
     "start_time": "2025-08-18T22:10:02.161395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest → AWS S3 (BLS/DataUSA)\n",
    "This notebook automates the retrieval of BLS productivity data and stores it in Amazon S3.  It uses the BLS Public API to retieve data. \n",
    "This sync version keeps S3 matched with the website and detects changes, updates, deletes automatically:\n",
    "* New files → get added.\n",
    "* Changed files → get updated.\n",
    "* Deleted files → get removed.\n",
    "\n",
    "### What's Covered\n",
    "- **Automated sync** from data api source to S3.\n",
    "- **No hardcoded file names** – dynamically scrapes the BLS file list.\n",
    "- **403 error handling** – uses a valid User-Agent to comply with BLS access policy.\n",
    "- **Cloud-based execution** – runs in Kaggle with secure secret management.\n",
    "- **Secrets used** – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY.\n",
    "- **Duplicate protection** – checks content hashes before uploading.\n",
    "\n",
    "### How It Works\n",
    "1. Fetch the current list of files from the BLS public directory.\n",
    "2. Download each file and compare its hash to the version in S3.\n",
    "3. Upload new or changed files to the configured S3 bucket.\n",
    "4. Skip unchanged files to save bandwidth and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8f15d",
   "metadata": {
    "papermill": {
     "duration": 0.002131,
     "end_time": "2025-08-18T22:10:02.169372",
     "exception": false,
     "start_time": "2025-08-18T22:10:02.167241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to AWS S3\n",
    "This notebook requires the following Python packages:  \n",
    "- boto3  \n",
    "- requests  \n",
    "- hashlib  \n",
    "- kaggle_secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50e2ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:10:02.175424Z",
     "iopub.status.busy": "2025-08-18T22:10:02.175114Z",
     "iopub.status.idle": "2025-08-18T22:10:04.184584Z",
     "shell.execute_reply": "2025-08-18T22:10:04.183739Z"
    },
    "papermill": {
     "duration": 2.01436,
     "end_time": "2025-08-18T22:10:04.186076",
     "exception": false,
     "start_time": "2025-08-18T22:10:02.171716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  40\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb0452",
   "metadata": {
    "papermill": {
     "duration": 0.00237,
     "end_time": "2025-08-18T22:10:04.191170",
     "exception": false,
     "start_time": "2025-08-18T22:10:04.188800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch BLS Data via API Key\n",
    "- Authenticate with a registered BLS API key to comply with access policies.  \n",
    "- Retrieve U.S. inflation data programmatically through the BLS Public API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710e8928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:10:04.197462Z",
     "iopub.status.busy": "2025-08-18T22:10:04.197094Z",
     "iopub.status.idle": "2025-08-18T22:10:04.994522Z",
     "shell.execute_reply": "2025-08-18T22:10:04.993465Z"
    },
    "papermill": {
     "duration": 0.802284,
     "end_time": "2025-08-18T22:10:04.996000",
     "exception": false,
     "start_time": "2025-08-18T22:10:04.193716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded bls_data.json to S3.\n"
     ]
    }
   ],
   "source": [
    "# API payload\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\n",
    "    \"seriesid\": [\"CUUR0000SA0\", \"SUUR0000SA0\"],  # You can customize this\n",
    "    \"startyear\": \"2020\",\n",
    "    \"endyear\": \"2024\",\n",
    "    \"registrationkey\": API_KEY\n",
    "})\n",
    "\n",
    "# Make request\n",
    "response = requests.post(\n",
    "    \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    \n",
    "    # Save locally\n",
    "    with open(\"bls_data.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Upload JSON file to S3 bucket\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=\"bls_data.json\",\n",
    "        Body=json.dumps(results, indent=2)\n",
    "    )\n",
    "    \n",
    "    print(\"Uploaded bls_data.json to S3.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f40dc",
   "metadata": {
    "papermill": {
     "duration": 0.002592,
     "end_time": "2025-08-18T22:10:05.001690",
     "exception": false,
     "start_time": "2025-08-18T22:10:04.999098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **API Data Pipeline – Send Files to S3 (No API Key Needed)**\n",
    "- **Custom User-Agent** to comply with BLS access rules and avoid 403 errors.  \n",
    "- **Checks for changes** by comparing file hashes before uploading.  \n",
    "- **Uploads only when updated**, reducing bandwidth usage and S3 storage costs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef97c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:10:05.008271Z",
     "iopub.status.busy": "2025-08-18T22:10:05.007827Z",
     "iopub.status.idle": "2025-08-18T22:10:07.209995Z",
     "shell.execute_reply": "2025-08-18T22:10:07.209086Z"
    },
    "papermill": {
     "duration": 2.207257,
     "end_time": "2025-08-18T22:10:07.211503",
     "exception": false,
     "start_time": "2025-08-18T22:10:05.004246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting..\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, re, hashlib, requests, boto3, botocore\n",
    "print(\"Starting..\")\n",
    "\n",
    "S3 = boto3.client(\"s3\", region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"))\n",
    "\n",
    "# -------- CONFIG --------\n",
    "#BUCKET = os.environ[\"BUCKET_NAME\"]\n",
    "PREFIX = os.getenv(\"BLS_PREFIX\", \"bls/pr/\")  # optional subfolder in S3\n",
    "BASE  = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "HDRS  = {\"User-Agent\": \"ScottSchmidt/1.0 (scott.schmidt1989@yahoo.com)\"}\n",
    "# Keep only real data/listing files (no parent links, no dirs)\n",
    "ALLOW = re.compile(r\"^[A-Za-z0-9._-]+$\")\n",
    "\n",
    "def list_source_files():\n",
    "    \"\"\"Scrape the BLS directory listing for file names.\"\"\"\n",
    "    resp = requests.get(BASE, headers=HDRS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    # Extract href=\"filename\" tokens\n",
    "    hrefs = re.findall(r'href=\"([^\"]+)\"', resp.text)\n",
    "    # Filter to plain files (no slashes, no query)\n",
    "    return [h for h in hrefs if ALLOW.match(h)]\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    \"\"\"List current S3 object keys under a prefix.\"\"\"\n",
    "    keys = []\n",
    "    paginator = S3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "    return keys\n",
    "\n",
    "def md5_hex(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def sync_one(filename: str):\n",
    "    \"\"\"Download from source, compare with S3, add/update/delete as needed.\"\"\"\n",
    "    url = BASE + filename\n",
    "    key = f\"{PREFIX}{filename}\"\n",
    "\n",
    "    r = requests.get(url, headers=HDRS, timeout=60)\n",
    "    if r.status_code == 404:\n",
    "        # Source file gone → ensure it's gone in S3\n",
    "        try:\n",
    "            S3.delete_object(Bucket=BUCKET, Key=key)\n",
    "            print(f\"Deleted from S3 (gone upstream): {filename}\")\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(f\"Not in S3 (already absent): {filename}\")\n",
    "            else:\n",
    "                raise\n",
    "        return\n",
    "\n",
    "    r.raise_for_status()\n",
    "    content = r.text\n",
    "    new_hash = md5_hex(content)\n",
    "\n",
    "    try:\n",
    "        obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "        existing = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        old_hash = md5_hex(existing)\n",
    "        if new_hash == old_hash:\n",
    "            print(f\"No change: {filename}\")\n",
    "            return\n",
    "        # changed\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content)\n",
    "        print(f\"Updated: {filename}\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            S3.put_object(Bucket=BUCKET, Key=key, Body=content)\n",
    "            print(f\"Added: {filename}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1) Get authoritative list from BLS\n",
    "    source_files = set(list_source_files())\n",
    "\n",
    "    # 2) Current objects in S3 (strip the prefix to compare names)\n",
    "    s3_keys = list_s3_keys(PREFIX)\n",
    "    s3_files = set(k[len(PREFIX):] for k in s3_keys if k.startswith(PREFIX))\n",
    "\n",
    "    # 3) Add/Update everything that exists upstream\n",
    "    for name in sorted(source_files):\n",
    "        sync_one(name)\n",
    "\n",
    "    # 4) Delete anything we have that upstream no longer has\n",
    "    extras = s3_files - source_files\n",
    "    for name in sorted(extras):\n",
    "        key = f\"{PREFIX}{name}\"\n",
    "        S3.delete_object(Bucket=BUCKET, Key=key)\n",
    "        print(f\"Deleted extra (not upstream): {name}\")\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": \"Full sync complete\"}\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8277aef",
   "metadata": {
    "papermill": {
     "duration": 0.002679,
     "end_time": "2025-08-18T22:10:07.217333",
     "exception": false,
     "start_time": "2025-08-18T22:10:07.214654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch and Upload BLS Data for Each Series ID\n",
    "This section goes through each BLS series ID, requests data from the API, \n",
    "and uploads it to S3 only if the data has changed since the last upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac5f6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:10:07.224216Z",
     "iopub.status.busy": "2025-08-18T22:10:07.223885Z",
     "iopub.status.idle": "2025-08-18T22:10:08.866632Z",
     "shell.execute_reply": "2025-08-18T22:10:08.865721Z"
    },
    "papermill": {
     "duration": 1.648144,
     "end_time": "2025-08-18T22:10:08.868167",
     "exception": false,
     "start_time": "2025-08-18T22:10:07.220023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching CUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  CUUR0000SA0.json\n",
      "Fetching SUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  SUUR0000SA0.json\n"
     ]
    }
   ],
   "source": [
    "# Series IDs you want to fetch\n",
    "series_ids = [\"CUUR0000SA0\", \"SUUR0000SA0\"]  \n",
    "\n",
    "# Loop through each series and fetch data from the BLS API\n",
    "for series_id in series_ids:\n",
    "    print(f\"Fetching {series_id} from BLS API...\")\n",
    "\n",
    "    # Prepare API request payload\n",
    "    payload = json.dumps({\n",
    "        \"seriesid\": [series_id],\n",
    "        \"startyear\": \"2020\",\n",
    "        \"endyear\": \"2024\",\n",
    "        \"registrationkey\": API_KEY\n",
    "    })\n",
    "\n",
    "    # Send POST request to the BLS API\n",
    "    response = requests.post(\n",
    "        \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "        data=payload,\n",
    "        headers={\"Content-type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    #  Skip if API request fails\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {series_id}: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    # Skip if API request fails\n",
    "    content = response.text\n",
    "    hash_new = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "    s3_key = f\"{series_id}.json\"\n",
    "\n",
    "    # Check if file already exists in S3 with same content\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "        existing = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        hash_existing = hashlib.md5(existing.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if hash_existing == hash_new:\n",
    "            print(series_id, \" Skipping. Unchanged series_id\")\n",
    "            continue\n",
    "    except ClientError:\n",
    "        pass  # File doesn't exist \n",
    "\n",
    "    # Upload to S3\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=content)\n",
    "    print(\"Uploaded to S3! \", s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f715a9c",
   "metadata": {
    "papermill": {
     "duration": 0.002595,
     "end_time": "2025-08-18T22:10:08.873703",
     "exception": false,
     "start_time": "2025-08-18T22:10:08.871108",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview BLS Data from S3\n",
    "This section retrieves a specific BLS JSON file from Amazon S3, \n",
    "converts it into a Pandas DataFrame, reorders the columns, \n",
    "and displays the first few rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3102da00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:10:08.880925Z",
     "iopub.status.busy": "2025-08-18T22:10:08.880184Z",
     "iopub.status.idle": "2025-08-18T22:10:10.846507Z",
     "shell.execute_reply": "2025-08-18T22:10:10.845682Z"
    },
    "papermill": {
     "duration": 1.971235,
     "end_time": "2025-08-18T22:10:10.847787",
     "exception": false,
     "start_time": "2025-08-18T22:10:08.876552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  (60, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>M12</td>\n",
       "      <td>315.605</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>M11</td>\n",
       "      <td>315.493</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>M10</td>\n",
       "      <td>315.664</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>M09</td>\n",
       "      <td>315.301</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>M08</td>\n",
       "      <td>314.796</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>M07</td>\n",
       "      <td>314.540</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>M06</td>\n",
       "      <td>314.175</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>M05</td>\n",
       "      <td>314.069</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>M04</td>\n",
       "      <td>313.548</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>M03</td>\n",
       "      <td>312.332</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024</td>\n",
       "      <td>M02</td>\n",
       "      <td>310.326</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024</td>\n",
       "      <td>M01</td>\n",
       "      <td>308.417</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023</td>\n",
       "      <td>M12</td>\n",
       "      <td>306.746</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023</td>\n",
       "      <td>M11</td>\n",
       "      <td>307.051</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023</td>\n",
       "      <td>M10</td>\n",
       "      <td>307.671</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023</td>\n",
       "      <td>M09</td>\n",
       "      <td>307.789</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023</td>\n",
       "      <td>M08</td>\n",
       "      <td>307.026</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>M07</td>\n",
       "      <td>305.691</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023</td>\n",
       "      <td>M06</td>\n",
       "      <td>305.109</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023</td>\n",
       "      <td>M05</td>\n",
       "      <td>304.127</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year period    value footnotes\n",
       "0   2024    M12  315.605      [{}]\n",
       "1   2024    M11  315.493      [{}]\n",
       "2   2024    M10  315.664      [{}]\n",
       "3   2024    M09  315.301      [{}]\n",
       "4   2024    M08  314.796      [{}]\n",
       "5   2024    M07  314.540      [{}]\n",
       "6   2024    M06  314.175      [{}]\n",
       "7   2024    M05  314.069      [{}]\n",
       "8   2024    M04  313.548      [{}]\n",
       "9   2024    M03  312.332      [{}]\n",
       "10  2024    M02  310.326      [{}]\n",
       "11  2024    M01  308.417      [{}]\n",
       "12  2023    M12  306.746      [{}]\n",
       "13  2023    M11  307.051      [{}]\n",
       "14  2023    M10  307.671      [{}]\n",
       "15  2023    M09  307.789      [{}]\n",
       "16  2023    M08  307.026      [{}]\n",
       "17  2023    M07  305.691      [{}]\n",
       "18  2023    M06  305.109      [{}]\n",
       "19  2023    M05  304.127      [{}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file content \n",
    "key = \"CUUR0000SA0.json\" # SUUR0000SA0\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "json_content = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract data into DataFrame\n",
    "series_data = json_content['Results']['series'][0]['data']\n",
    "df = pd.DataFrame(series_data)\n",
    "df = df[[\"year\", \"period\", \"value\", \"footnotes\"]]\n",
    "print(\"DataFrame Shape: \", df.shape)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.972636,
   "end_time": "2025-08-18T22:10:11.570055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T22:09:57.597419",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
