{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e71c7a",
   "metadata": {
    "papermill": {
     "duration": 0.002152,
     "end_time": "2025-08-19T03:46:40.335443",
     "exception": false,
     "start_time": "2025-08-19T03:46:40.333291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest & SYNC → AWS S3 \n",
    "This notebook automates the retrieval of BLS/DataUSA data and stores it in Amazon S3.  It uses the BLS Public API to retieve data. \n",
    "This sync version keeps S3 matched with the website and detects changes, updates, deletes automatically:\n",
    "* New files → get added.\n",
    "* Changed files → get updated.\n",
    "* Deleted files → get removed.\n",
    "  \n",
    "**[View Notebook (Foundational Version)](https://github.com/ScottySchmidt/AWS_DataEngineer_API/blob/main/01-ingest-apis-to-s3.ipynb)**\n",
    "\n",
    "This version laid the groundwork for the improved **Sync Version**, which now mirrors the full BLS directory with adds/updates/deletes and supports optional targeted series syncs.\n",
    "\n",
    "### What's Covered\n",
    "- **Automated sync** from data api source to S3.\n",
    "- **No hardcoded file names** – dynamically scrapes the BLS file list.\n",
    "- **403 error handling** – uses a valid User-Agent to comply with BLS access policy.\n",
    "- **Cloud-based execution** – runs in Kaggle with secure secret management.\n",
    "- **Secrets used** – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY.\n",
    "- **Duplicate protection** – checks content hashes before uploading.\n",
    "\n",
    "### How It Works\n",
    "1. Fetch the current list of files from the BLS public directory.\n",
    "2. Download each file and compare its hash to the version in S3.\n",
    "3. Upload new or changed files to the configured S3 bucket.\n",
    "4. Skip unchanged files to save bandwidth and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8744aa50",
   "metadata": {
    "papermill": {
     "duration": 0.001302,
     "end_time": "2025-08-19T03:46:40.338624",
     "exception": false,
     "start_time": "2025-08-19T03:46:40.337322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sync BLS Data to S3\n",
    "- Script watches for new, changed, or deleted files.  \n",
    "- Uses a custom ID so BLS doesn’t block us.  \n",
    "- Checks if a file is different before uploading.  \n",
    "- Only uploads when needed to save space and time.  \n",
    "- Keeps S3 matched up with the BLS site (using API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9a0624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:46:40.343329Z",
     "iopub.status.busy": "2025-08-19T03:46:40.343026Z",
     "iopub.status.idle": "2025-08-19T03:46:43.467902Z",
     "shell.execute_reply": "2025-08-19T03:46:43.466968Z"
    },
    "papermill": {
     "duration": 3.129073,
     "end_time": "2025-08-19T03:46:43.469451",
     "exception": false,
     "start_time": "2025-08-19T03:46:40.340378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  43\n",
      "filename: bls_CUUR0000SA0-SUUR0000SA0.json\n",
      "s3 key: bls/api/bls_CUUR0000SA0-SUUR0000SA0.json\n",
      "got data\n",
      "saved: bls_CUUR0000SA0-SUUR0000SA0.json\n",
      "uploaded: bls/api/bls_CUUR0000SA0-SUUR0000SA0.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)\n",
    "\n",
    "SERIES_IDS = os.getenv(\"SERIES_IDS\", \"CUUR0000SA0,SUUR0000SA0\")\n",
    "SERIES_IDS = [s.strip() for s in SERIES_IDS.split(\",\") if s.strip()]\n",
    "\n",
    "filename = f\"bls_{'-'.join(sorted(SERIES_IDS))}.json\"   # ← deterministic, no hardcode\n",
    "s3_key   = f\"bls/api/{filename}\"\n",
    "\n",
    "print(\"filename:\", filename)\n",
    "print(\"s3 key:\", s3_key)\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"User-Agent\": os.getenv(\"USER_AGENT\", \"ScottSchmidt/1.0 (email)\")\n",
    "}\n",
    "payload = {\n",
    "    \"seriesid\": SERIES_IDS,\n",
    "    \"registrationkey\": API_KEY\n",
    "}\n",
    "\n",
    "resp = requests.post(\n",
    "    \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "    data=json.dumps(payload),\n",
    "    headers=headers,\n",
    "    timeout=60\n",
    ")\n",
    "if resp.status_code != 200:\n",
    "    raise RuntimeError(f\"BLS error {resp.status_code}: {resp.text[:200]}\")\n",
    "data = resp.json()\n",
    "print(\"got data\")\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(\"saved:\", filename)\n",
    "\n",
    "# Add data to S3 bucket:\n",
    "s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=json.dumps(data, indent=2))\n",
    "print(\"uploaded:\", s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d213d3",
   "metadata": {
    "papermill": {
     "duration": 0.001488,
     "end_time": "2025-08-19T03:46:43.472855",
     "exception": false,
     "start_time": "2025-08-19T03:46:43.471367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview Synced BLS Data\n",
    "Load the full set of BLS JSON files from S3 (kept in sync with the source) into a single DataFrame for analysis."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.321717,
   "end_time": "2025-08-19T03:46:43.994130",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-19T03:46:35.672413",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
