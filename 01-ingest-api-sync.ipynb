{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58ce36b",
   "metadata": {
    "papermill": {
     "duration": 0.003292,
     "end_time": "2025-08-19T00:38:29.323091",
     "exception": false,
     "start_time": "2025-08-19T00:38:29.319799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest & SYNC → AWS S3 \n",
    "This notebook automates the retrieval of BLS/DataUSA data and stores it in Amazon S3.  It uses the BLS Public API to retieve data. \n",
    "This sync version keeps S3 matched with the website and detects changes, updates, deletes automatically:\n",
    "* New files → get added.\n",
    "* Changed files → get updated.\n",
    "* Deleted files → get removed.\n",
    "  \n",
    "**[View Notebook (Foundational Version)](https://github.com/ScottySchmidt/AWS_DataEngineer_API/blob/main/01-ingest-apis-to-s3.ipynb)**\n",
    "\n",
    "This version laid the groundwork for the improved **Sync Version**, which now mirrors the full BLS directory with adds/updates/deletes and supports optional targeted series syncs.\n",
    "\n",
    "### What's Covered\n",
    "- **Automated sync** from data api source to S3.\n",
    "- **No hardcoded file names** – dynamically scrapes the BLS file list.\n",
    "- **403 error handling** – uses a valid User-Agent to comply with BLS access policy.\n",
    "- **Cloud-based execution** – runs in Kaggle with secure secret management.\n",
    "- **Secrets used** – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY.\n",
    "- **Duplicate protection** – checks content hashes before uploading.\n",
    "\n",
    "### How It Works\n",
    "1. Fetch the current list of files from the BLS public directory.\n",
    "2. Download each file and compare its hash to the version in S3.\n",
    "3. Upload new or changed files to the configured S3 bucket.\n",
    "4. Skip unchanged files to save bandwidth and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f6229",
   "metadata": {
    "papermill": {
     "duration": 0.002249,
     "end_time": "2025-08-19T00:38:29.328051",
     "exception": false,
     "start_time": "2025-08-19T00:38:29.325802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to AWS S3\n",
    "This notebook requires the following Python packages:  \n",
    "- boto3  \n",
    "- requests  \n",
    "- hashlib  \n",
    "- kaggle_secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdcdae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T00:38:29.334896Z",
     "iopub.status.busy": "2025-08-19T00:38:29.334454Z",
     "iopub.status.idle": "2025-08-19T00:38:32.293613Z",
     "shell.execute_reply": "2025-08-19T00:38:32.292380Z"
    },
    "papermill": {
     "duration": 2.964744,
     "end_time": "2025-08-19T00:38:32.295595",
     "exception": false,
     "start_time": "2025-08-19T00:38:29.330851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Keys visible: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "# Load secrets: Kaggle first, then env vars\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    AWS_ACCESS_KEY_ID     = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION            = secrets.get_secret(\"AWS_REGION\") or os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "    BUCKET_NAME           = secrets.get_secret(\"BUCKET_NAME\") or os.environ[\"BUCKET_NAME\"]\n",
    "except Exception:\n",
    "    AWS_ACCESS_KEY_ID     = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    AWS_REGION            = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "    BUCKET_NAME           = os.environ[\"BUCKET_NAME\"]\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "def s3_health_check(bucket):\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "        page = s3.list_objects_v2(Bucket=bucket, MaxKeys=1)\n",
    "        print(\"S3 connection successful. Keys visible:\", page.get(\"KeyCount\", 0))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"S3 check failed:\", str(e))\n",
    "        return False\n",
    "\n",
    "if not s3_health_check(BUCKET_NAME):\n",
    "    raise SystemExit(\"Fix your S3 setup before running sync.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86533007",
   "metadata": {
    "papermill": {
     "duration": 0.002229,
     "end_time": "2025-08-19T00:38:32.300593",
     "exception": false,
     "start_time": "2025-08-19T00:38:32.298364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Sync BLS Data to S3**\n",
    "- Script watches for new, changed, or deleted files.  \n",
    "- Uses a custom ID so BLS doesn’t block us.  \n",
    "- Checks if a file is different before uploading.  \n",
    "- Only uploads when needed to save space and time.  \n",
    "- Keeps S3 matched up with the BLS site.  \n",
    "\n",
    "## Get BLS Data with an API Key\n",
    "- Use your API key to log in the right way.  \n",
    "- Pull U.S. inflation data straight from the BLS.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9480af15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T00:38:32.308402Z",
     "iopub.status.busy": "2025-08-19T00:38:32.307904Z",
     "iopub.status.idle": "2025-08-19T00:38:34.647167Z",
     "shell.execute_reply": "2025-08-19T00:38:34.645767Z"
    },
    "papermill": {
     "duration": 2.345259,
     "end_time": "2025-08-19T00:38:34.649133",
     "exception": false,
     "start_time": "2025-08-19T00:38:32.303874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, hashlib, requests, boto3, botocore\n",
    "print(\"starting\")\n",
    "\n",
    "# AWS / env\n",
    "S3          = boto3.client(\"s3\", region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"))\n",
    "#BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "USER_AGENT  = os.getenv(\"USER_AGENT\", \"ScottSchmidt/1.0 (email)\")\n",
    "SERIES_IDS  = [s.strip() for s in os.getenv(\"SERIES_IDS\", \"\").split(\",\") if s.strip()]\n",
    "BLS_API_KEY = os.getenv(\"BLS_API_KEY\")  # only needed if SERIES_IDS is set\n",
    "\n",
    "# Bulk sync (BLS pr/ directory)\n",
    "BASE_URL   = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "BLS_PREFIX = os.getenv(\"BLS_PREFIX\", \"bls/pr/\")\n",
    "ALLOW      = re.compile(r\"^[A-Za-z0-9._-]+$\")\n",
    "\n",
    "def md5_text(t: str) -> str:\n",
    "    return hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def md5_bytes(b: bytes) -> str:\n",
    "    return hashlib.md5(b).hexdigest()\n",
    "\n",
    "def list_upstream_files():\n",
    "    r = requests.get(BASE_URL, headers={\"User-Agent\": USER_AGENT}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    hrefs = re.findall(r'href=\"([^\"]+)\"', r.text)\n",
    "    return [h for h in hrefs if ALLOW.match(h)]\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    keys, paginator = [], S3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET_NAME, Prefix=prefix):\n",
    "        for obj in (page.get(\"Contents\") or []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "    return keys\n",
    "\n",
    "def sync_bulk_file(name: str):\n",
    "    key = f\"{BLS_PREFIX}{name}\"\n",
    "    r = requests.get(BASE_URL + name, headers={\"User-Agent\": USER_AGENT}, timeout=60)\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        try:\n",
    "            S3.delete_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            print(f\"deleted: {name}\")\n",
    "            return \"deleted\"\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] != \"NoSuchKey\":\n",
    "                raise\n",
    "            return \"absent\"\n",
    "\n",
    "    r.raise_for_status()\n",
    "    content = r.text\n",
    "    new_hash = md5_text(content)\n",
    "\n",
    "    try:\n",
    "        obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "        if md5_text(obj[\"Body\"].read().decode(\"utf-8\")) == new_hash:\n",
    "            print(f\"no-change: {name}\")\n",
    "            return \"unchanged\"\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"text/plain\")\n",
    "        print(f\"updated: {name}\")\n",
    "        return \"updated\"\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"text/plain\")\n",
    "            print(f\"added: {name}\")\n",
    "            return \"added\"\n",
    "        raise\n",
    "\n",
    "def run_bulk_full_sync():\n",
    "    upstream = set(list_upstream_files())\n",
    "    s3_files = set(k[len(BLS_PREFIX):] for k in list_s3_keys(BLS_PREFIX) if k.startswith(BLS_PREFIX))\n",
    "\n",
    "    added = updated = unchanged = 0\n",
    "    for name in sorted(upstream):\n",
    "        res = sync_bulk_file(name)\n",
    "        if res == \"added\": added += 1\n",
    "        elif res == \"updated\": updated += 1\n",
    "        elif res == \"unchanged\": unchanged += 1\n",
    "\n",
    "    extras = s3_files - upstream\n",
    "    deleted = 0\n",
    "    for name in sorted(extras):\n",
    "        S3.delete_object(Bucket=BUCKET_NAME, Key=f\"{BLS_PREFIX}{name}\")\n",
    "        print(f\"deleted-extra: {name}\")\n",
    "        deleted += 1\n",
    "\n",
    "    return {\"added\": added, \"updated\": updated, \"unchanged\": unchanged, \"deleted\": deleted}\n",
    "\n",
    "def run_series_sync(series_ids):\n",
    "    if not series_ids:\n",
    "        return {\"upserted\": 0, \"unchanged\": 0, \"skipped\": True}\n",
    "    if not BLS_API_KEY:\n",
    "        return {\"error\": \"BLS_API_KEY missing\", \"upserted\": 0, \"unchanged\": 0}\n",
    "\n",
    "    upserted = unchanged = 0\n",
    "    for sid in series_ids:\n",
    "        print(f\"series: {sid}\")\n",
    "        payload = json.dumps({\"seriesid\": [sid], \"registrationkey\": BLS_API_KEY})\n",
    "        r = requests.post(\n",
    "            \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "            data=payload,\n",
    "            headers={\"Content-type\": \"application/json\"},\n",
    "            timeout=60,\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            print(f\"series-fail: {sid} {r.status_code}\")\n",
    "            continue\n",
    "\n",
    "        content = r.text.encode(\"utf-8\")\n",
    "        key = f\"bls/series/{sid}.json\"\n",
    "\n",
    "        try:\n",
    "            obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            if md5_bytes(obj[\"Body\"].read()) == md5_bytes(content):\n",
    "                print(f\"series-no-change: {sid}\")\n",
    "                unchanged += 1\n",
    "                continue\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] != \"NoSuchKey\":\n",
    "                raise\n",
    "\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"application/json\")\n",
    "        print(f\"series-upserted: {sid}\")\n",
    "        upserted += 1\n",
    "\n",
    "    return {\"upserted\": upserted, \"unchanged\": unchanged}\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"start\")\n",
    "    bulk   = run_bulk_full_sync()\n",
    "    series = run_series_sync(SERIES_IDS)\n",
    "    summary = {\"bulk\": bulk, \"series\": series}\n",
    "    print(json.dumps(summary))\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps(summary)}\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d2250",
   "metadata": {
    "papermill": {
     "duration": 0.002784,
     "end_time": "2025-08-19T00:38:34.654805",
     "exception": false,
     "start_time": "2025-08-19T00:38:34.652021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview Synced BLS Data\n",
    "Load the full set of BLS JSON files from S3 (kept in sync with the source) into a single DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db75fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T00:38:34.661739Z",
     "iopub.status.busy": "2025-08-19T00:38:34.661374Z",
     "iopub.status.idle": "2025-08-19T00:38:37.269047Z",
     "shell.execute_reply": "2025-08-19T00:38:37.267963Z"
    },
    "papermill": {
     "duration": 2.613251,
     "end_time": "2025-08-19T00:38:37.270800",
     "exception": false,
     "start_time": "2025-08-19T00:38:34.657549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# list all json files under your prefix\n",
    "objs = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=BLS_PREFIX).get(\"Contents\", [])\n",
    "\n",
    "frames = []\n",
    "for o in objs:\n",
    "    key = o[\"Key\"]\n",
    "    if not key.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "    content = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "    data = json.loads(content)\n",
    "\n",
    "    series = data.get(\"Results\", {}).get(\"series\")\n",
    "    if not series:\n",
    "        continue\n",
    "\n",
    "    records = series[0].get(\"data\", [])\n",
    "    if not records:\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    cols = [c for c in [\"year\", \"period\", \"value\", \"footnotes\"] if c in df.columns]\n",
    "    df = df[cols]\n",
    "    df[\"source_file\"] = key\n",
    "    frames.append(df)\n",
    "\n",
    "# one DataFrame with all series files\n",
    "all_data = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "print(\"rows:\", len(all_data))\n",
    "all_data.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.387346,
   "end_time": "2025-08-19T00:38:38.299049",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-19T00:38:23.911703",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
