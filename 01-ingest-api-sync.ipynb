{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4c0103",
   "metadata": {
    "papermill": {
     "duration": 0.003224,
     "end_time": "2025-08-18T22:52:55.337104",
     "exception": false,
     "start_time": "2025-08-18T22:52:55.333880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest → AWS S3 (BLS/DataUSA)\n",
    "This notebook automates the retrieval of BLS productivity data and stores it in Amazon S3.  It uses the BLS Public API to retieve data. \n",
    "This sync version keeps S3 matched with the website and detects changes, updates, deletes automatically:\n",
    "* New files → get added.\n",
    "* Changed files → get updated.\n",
    "* Deleted files → get removed.\n",
    "  \n",
    "[View Notebook (Foundational Version)](https://github.com/ScottySchmidt/AWS_DataEngineer_API/blob/main/01-ingest-apis-to-s3.ipynb)\n",
    "This version laid the groundwork for the improved **Sync Version**, which now mirrors the full BLS directory with adds/updates/deletes and supports optional targeted series syncs.\n",
    "\n",
    "### What's Covered\n",
    "- **Automated sync** from data api source to S3.\n",
    "- **No hardcoded file names** – dynamically scrapes the BLS file list.\n",
    "- **403 error handling** – uses a valid User-Agent to comply with BLS access policy.\n",
    "- **Cloud-based execution** – runs in Kaggle with secure secret management.\n",
    "- **Secrets used** – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY.\n",
    "- **Duplicate protection** – checks content hashes before uploading.\n",
    "\n",
    "### How It Works\n",
    "1. Fetch the current list of files from the BLS public directory.\n",
    "2. Download each file and compare its hash to the version in S3.\n",
    "3. Upload new or changed files to the configured S3 bucket.\n",
    "4. Skip unchanged files to save bandwidth and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3987d62",
   "metadata": {
    "papermill": {
     "duration": 0.002281,
     "end_time": "2025-08-18T22:52:55.342385",
     "exception": false,
     "start_time": "2025-08-18T22:52:55.340104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to AWS S3\n",
    "This notebook requires the following Python packages:  \n",
    "- boto3  \n",
    "- requests  \n",
    "- hashlib  \n",
    "- kaggle_secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a3f862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:52:55.348665Z",
     "iopub.status.busy": "2025-08-18T22:52:55.348360Z",
     "iopub.status.idle": "2025-08-18T22:52:57.014940Z",
     "shell.execute_reply": "2025-08-18T22:52:57.013902Z"
    },
    "papermill": {
     "duration": 1.671771,
     "end_time": "2025-08-18T22:52:57.016603",
     "exception": false,
     "start_time": "2025-08-18T22:52:55.344832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  42\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c9ef2",
   "metadata": {
    "papermill": {
     "duration": 0.002355,
     "end_time": "2025-08-18T22:52:57.021786",
     "exception": false,
     "start_time": "2025-08-18T22:52:57.019431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch BLS Data via API Key\n",
    "- Authenticate with a registered BLS API key to comply with access policies.  \n",
    "- Retrieve U.S. inflation data programmatically through the BLS Public API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5948488d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:52:57.028601Z",
     "iopub.status.busy": "2025-08-18T22:52:57.027839Z",
     "iopub.status.idle": "2025-08-18T22:52:57.463803Z",
     "shell.execute_reply": "2025-08-18T22:52:57.462573Z"
    },
    "papermill": {
     "duration": 0.44136,
     "end_time": "2025-08-18T22:52:57.465762",
     "exception": false,
     "start_time": "2025-08-18T22:52:57.024402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded bls_data.json to S3.\n"
     ]
    }
   ],
   "source": [
    "# API payload\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\n",
    "    \"seriesid\": [\"CUUR0000SA0\", \"SUUR0000SA0\"],  # You can customize this\n",
    "    \"startyear\": \"2020\",\n",
    "    \"endyear\": \"2024\",\n",
    "    \"registrationkey\": API_KEY\n",
    "})\n",
    "\n",
    "# Make request\n",
    "response = requests.post(\n",
    "    \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    \n",
    "    # Save locally\n",
    "    with open(\"bls_data.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Upload JSON file to S3 bucket\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=\"bls_data.json\",\n",
    "        Body=json.dumps(results, indent=2)\n",
    "    )\n",
    "    \n",
    "    print(\"Uploaded bls_data.json to S3.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e563d",
   "metadata": {
    "papermill": {
     "duration": 0.003675,
     "end_time": "2025-08-18T22:52:57.473740",
     "exception": false,
     "start_time": "2025-08-18T22:52:57.470065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **API Data Pipeline – Send Files to S3 (No API Key Needed)**\n",
    "- **No hardcoded file names** — script adapts to new or removed files automatically.  \n",
    "- **Custom User-Agent** to comply with BLS access rules and avoid 403 errors.  \n",
    "- **Checks for changes** by comparing file hashes before uploading.  \n",
    "- **Uploads only when updated**, reducing bandwidth usage and S3 storage costs.  \n",
    "- **Syncs with source** — handles added or removed files, and avoids re-uploading duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db6ff6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:52:57.481965Z",
     "iopub.status.busy": "2025-08-18T22:52:57.481642Z",
     "iopub.status.idle": "2025-08-18T22:52:59.708211Z",
     "shell.execute_reply": "2025-08-18T22:52:59.707195Z"
    },
    "papermill": {
     "duration": 2.233013,
     "end_time": "2025-08-18T22:52:59.710036",
     "exception": false,
     "start_time": "2025-08-18T22:52:57.477023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re, json, hashlib, requests, boto3, botocore\n",
    "\n",
    "# AWS / env\n",
    "S3          = boto3.client(\"s3\", region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"))\n",
    "#BUCKET_NAME = os.environ[\"BUCKET_NAME\"]\n",
    "USER_AGENT  = os.getenv(\"USER_AGENT\", \"ScottSchmidt/1.0 (email)\")\n",
    "SERIES_IDS  = [s.strip() for s in os.getenv(\"SERIES_IDS\", \"\").split(\",\") if s.strip()]\n",
    "BLS_API_KEY = os.getenv(\"BLS_API_KEY\")  # only needed if SERIES_IDS is set\n",
    "\n",
    "# Bulk sync (BLS pr/ directory)\n",
    "BASE_URL   = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "BLS_PREFIX = os.getenv(\"BLS_PREFIX\", \"bls/pr/\")\n",
    "ALLOW      = re.compile(r\"^[A-Za-z0-9._-]+$\")\n",
    "\n",
    "def md5_text(t: str) -> str:\n",
    "    return hashlib.md5(t.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def md5_bytes(b: bytes) -> str:\n",
    "    return hashlib.md5(b).hexdigest()\n",
    "\n",
    "def list_upstream_files():\n",
    "    r = requests.get(BASE_URL, headers={\"User-Agent\": USER_AGENT}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    hrefs = re.findall(r'href=\"([^\"]+)\"', r.text)\n",
    "    return [h for h in hrefs if ALLOW.match(h)]\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    keys, paginator = [], S3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET_NAME, Prefix=prefix):\n",
    "        for obj in (page.get(\"Contents\") or []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "    return keys\n",
    "\n",
    "def sync_bulk_file(name: str):\n",
    "    key = f\"{BLS_PREFIX}{name}\"\n",
    "    r = requests.get(BASE_URL + name, headers={\"User-Agent\": USER_AGENT}, timeout=60)\n",
    "\n",
    "    if r.status_code == 404:\n",
    "        try:\n",
    "            S3.delete_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            print(f\"deleted: {name}\")\n",
    "            return \"deleted\"\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] != \"NoSuchKey\":\n",
    "                raise\n",
    "            return \"absent\"\n",
    "\n",
    "    r.raise_for_status()\n",
    "    content = r.text\n",
    "    new_hash = md5_text(content)\n",
    "\n",
    "    try:\n",
    "        obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "        if md5_text(obj[\"Body\"].read().decode(\"utf-8\")) == new_hash:\n",
    "            print(f\"no-change: {name}\")\n",
    "            return \"unchanged\"\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"text/plain\")\n",
    "        print(f\"updated: {name}\")\n",
    "        return \"updated\"\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"text/plain\")\n",
    "            print(f\"added: {name}\")\n",
    "            return \"added\"\n",
    "        raise\n",
    "\n",
    "def run_bulk_full_sync():\n",
    "    upstream = set(list_upstream_files())\n",
    "    s3_files = set(k[len(BLS_PREFIX):] for k in list_s3_keys(BLS_PREFIX) if k.startswith(BLS_PREFIX))\n",
    "\n",
    "    added = updated = unchanged = 0\n",
    "    for name in sorted(upstream):\n",
    "        res = sync_bulk_file(name)\n",
    "        if res == \"added\": added += 1\n",
    "        elif res == \"updated\": updated += 1\n",
    "        elif res == \"unchanged\": unchanged += 1\n",
    "\n",
    "    extras = s3_files - upstream\n",
    "    deleted = 0\n",
    "    for name in sorted(extras):\n",
    "        S3.delete_object(Bucket=BUCKET_NAME, Key=f\"{BLS_PREFIX}{name}\")\n",
    "        print(f\"deleted-extra: {name}\")\n",
    "        deleted += 1\n",
    "\n",
    "    return {\"added\": added, \"updated\": updated, \"unchanged\": unchanged, \"deleted\": deleted}\n",
    "\n",
    "def run_series_sync(series_ids):\n",
    "    if not series_ids:\n",
    "        return {\"upserted\": 0, \"unchanged\": 0, \"skipped\": True}\n",
    "    if not BLS_API_KEY:\n",
    "        return {\"error\": \"BLS_API_KEY missing\", \"upserted\": 0, \"unchanged\": 0}\n",
    "\n",
    "    upserted = unchanged = 0\n",
    "    for sid in series_ids:\n",
    "        print(f\"series: {sid}\")\n",
    "        payload = json.dumps({\"seriesid\": [sid], \"registrationkey\": BLS_API_KEY})\n",
    "        r = requests.post(\n",
    "            \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "            data=payload,\n",
    "            headers={\"Content-type\": \"application/json\"},\n",
    "            timeout=60,\n",
    "        )\n",
    "        if r.status_code != 200:\n",
    "            print(f\"series-fail: {sid} {r.status_code}\")\n",
    "            continue\n",
    "\n",
    "        content = r.text.encode(\"utf-8\")\n",
    "        key = f\"bls/series/{sid}.json\"\n",
    "\n",
    "        try:\n",
    "            obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "            if md5_bytes(obj[\"Body\"].read()) == md5_bytes(content):\n",
    "                print(f\"series-no-change: {sid}\")\n",
    "                unchanged += 1\n",
    "                continue\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] != \"NoSuchKey\":\n",
    "                raise\n",
    "\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content, ContentType=\"application/json\")\n",
    "        print(f\"series-upserted: {sid}\")\n",
    "        upserted += 1\n",
    "\n",
    "    return {\"upserted\": upserted, \"unchanged\": unchanged}\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"start\")\n",
    "    bulk   = run_bulk_full_sync()\n",
    "    series = run_series_sync(SERIES_IDS)\n",
    "    summary = {\"bulk\": bulk, \"series\": series}\n",
    "    print(json.dumps(summary))\n",
    "    print(\"done\")\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps(summary)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faaafc7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:52:59.718230Z",
     "iopub.status.busy": "2025-08-18T22:52:59.717879Z",
     "iopub.status.idle": "2025-08-18T22:53:01.746495Z",
     "shell.execute_reply": "2025-08-18T22:53:01.745459Z"
    },
    "papermill": {
     "duration": 2.034741,
     "end_time": "2025-08-18T22:53:01.748079",
     "exception": false,
     "start_time": "2025-08-18T22:52:59.713338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting..\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, re, hashlib, requests, boto3, botocore\n",
    "print(\"Starting..\")\n",
    "\n",
    "# S3 client (defaults to us-east-1 if AWS_REGION not set)\n",
    "S3 = boto3.client(\"s3\", region_name=os.getenv(\"AWS_REGION\", \"us-east-1\"))\n",
    "\n",
    "# CONFIG\n",
    "PREFIX = os.getenv(\"BLS_PREFIX\", \"bls/pr/\")  # optional subfolder in S3\n",
    "BASE   = \"https://download.bls.gov/pub/time.series/pr/\"\n",
    "HDRS   = {\"User-Agent\": \"ScottSchmidt/1.0 (scott.schmidt1989@yahoo.com)\"}\n",
    "\n",
    "# Only allow normal file names (skip folders, junk, or query strings)\n",
    "ALLOW = re.compile(r\"^[A-Za-z0-9._-]+$\")\n",
    "\n",
    "\n",
    "def list_source_files():\n",
    "    \"\"\"Get the list of all files available in the BLS directory.\"\"\"\n",
    "    resp = requests.get(BASE, headers=HDRS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    hrefs = re.findall(r'href=\"([^\"]+)\"', resp.text)\n",
    "    return [h for h in hrefs if ALLOW.match(h)]\n",
    "\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    \"\"\"Get all files currently stored in S3 under our prefix.\"\"\"\n",
    "    keys = []\n",
    "    paginator = S3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            keys.append(obj[\"Key\"])\n",
    "    return keys\n",
    "\n",
    "\n",
    "def md5_hex(text: str) -> str:\n",
    "    \"\"\"Quick way to hash file contents so we can check for changes.\"\"\"\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def sync_one(filename: str):\n",
    "    \"\"\"Sync a single file between BLS and S3 (add, update, or delete).\"\"\"\n",
    "    url = BASE + filename\n",
    "    key = f\"{PREFIX}{filename}\"\n",
    "\n",
    "    r = requests.get(url, headers=HDRS, timeout=60)\n",
    "\n",
    "    # If file is gone upstream, remove it from S3 too\n",
    "    if r.status_code == 404:\n",
    "        try:\n",
    "            S3.delete_object(Bucket=BUCKET, Key=key)\n",
    "            print(f\"Deleted from S3 (removed at source): {filename}\")\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(f\"Already gone in S3: {filename}\")\n",
    "            else:\n",
    "                raise\n",
    "        return\n",
    "\n",
    "    # File exists → compare hashes\n",
    "    r.raise_for_status()\n",
    "    content = r.text\n",
    "    new_hash = md5_hex(content)\n",
    "\n",
    "    try:\n",
    "        obj = S3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "        old_hash = md5_hex(obj[\"Body\"].read().decode(\"utf-8\"))\n",
    "        if new_hash == old_hash:\n",
    "            print(f\"No change: {filename}\")\n",
    "            return\n",
    "        # Content changed → update S3\n",
    "        S3.put_object(Bucket=BUCKET_NAME, Key=key, Body=content)\n",
    "        print(f\"Updated: {filename}\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            # New file → add it to S3\n",
    "            S3.put_object(Bucket=BUCKET, Key=key, Body=content)\n",
    "            print(f\"Added: {filename}\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1) List all files from BLS (the source of truth)\n",
    "    source_files = set(list_source_files())\n",
    "\n",
    "    # 2) List all files we currently have in S3\n",
    "    s3_keys = list_s3_keys(PREFIX)\n",
    "    s3_files = set(k[len(PREFIX):] for k in s3_keys if k.startswith(PREFIX))\n",
    "\n",
    "    # 3) Add or update anything that exists upstream\n",
    "    for name in sorted(source_files):\n",
    "        sync_one(name)\n",
    "\n",
    "    # 4) Remove anything in S3 that no longer exists upstream\n",
    "    extras = s3_files - source_files\n",
    "    for name in sorted(extras):\n",
    "        key = f\"{PREFIX}{name}\"\n",
    "        S3.delete_object(Bucket=BUCKET, Key=key)\n",
    "        print(f\"Deleted extra (not in source): {name}\")\n",
    "\n",
    "    return {\"statusCode\": 200, \"body\": \"Full sync complete\"}\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9d0ba",
   "metadata": {
    "papermill": {
     "duration": 0.00256,
     "end_time": "2025-08-18T22:53:01.753782",
     "exception": false,
     "start_time": "2025-08-18T22:53:01.751222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview BLS Data from S3\n",
    "This section retrieves a specific BLS JSON file from Amazon S3, \n",
    "converts it into a Pandas DataFrame, reorders the columns, \n",
    "and displays the first few rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6354eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T22:53:01.760903Z",
     "iopub.status.busy": "2025-08-18T22:53:01.760515Z",
     "iopub.status.idle": "2025-08-18T22:53:04.071736Z",
     "shell.execute_reply": "2025-08-18T22:53:04.070821Z"
    },
    "papermill": {
     "duration": 2.316619,
     "end_time": "2025-08-18T22:53:04.073093",
     "exception": false,
     "start_time": "2025-08-18T22:53:01.756474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  (60, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>M12</td>\n",
       "      <td>315.605</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>M11</td>\n",
       "      <td>315.493</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>M10</td>\n",
       "      <td>315.664</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>M09</td>\n",
       "      <td>315.301</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>M08</td>\n",
       "      <td>314.796</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>M07</td>\n",
       "      <td>314.540</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>M06</td>\n",
       "      <td>314.175</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>M05</td>\n",
       "      <td>314.069</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>M04</td>\n",
       "      <td>313.548</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>M03</td>\n",
       "      <td>312.332</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024</td>\n",
       "      <td>M02</td>\n",
       "      <td>310.326</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024</td>\n",
       "      <td>M01</td>\n",
       "      <td>308.417</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023</td>\n",
       "      <td>M12</td>\n",
       "      <td>306.746</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023</td>\n",
       "      <td>M11</td>\n",
       "      <td>307.051</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023</td>\n",
       "      <td>M10</td>\n",
       "      <td>307.671</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023</td>\n",
       "      <td>M09</td>\n",
       "      <td>307.789</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023</td>\n",
       "      <td>M08</td>\n",
       "      <td>307.026</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>M07</td>\n",
       "      <td>305.691</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023</td>\n",
       "      <td>M06</td>\n",
       "      <td>305.109</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023</td>\n",
       "      <td>M05</td>\n",
       "      <td>304.127</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year period    value footnotes\n",
       "0   2024    M12  315.605      [{}]\n",
       "1   2024    M11  315.493      [{}]\n",
       "2   2024    M10  315.664      [{}]\n",
       "3   2024    M09  315.301      [{}]\n",
       "4   2024    M08  314.796      [{}]\n",
       "5   2024    M07  314.540      [{}]\n",
       "6   2024    M06  314.175      [{}]\n",
       "7   2024    M05  314.069      [{}]\n",
       "8   2024    M04  313.548      [{}]\n",
       "9   2024    M03  312.332      [{}]\n",
       "10  2024    M02  310.326      [{}]\n",
       "11  2024    M01  308.417      [{}]\n",
       "12  2023    M12  306.746      [{}]\n",
       "13  2023    M11  307.051      [{}]\n",
       "14  2023    M10  307.671      [{}]\n",
       "15  2023    M09  307.789      [{}]\n",
       "16  2023    M08  307.026      [{}]\n",
       "17  2023    M07  305.691      [{}]\n",
       "18  2023    M06  305.109      [{}]\n",
       "19  2023    M05  304.127      [{}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file content \n",
    "key = \"CUUR0000SA0.json\" # SUUR0000SA0\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "json_content = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract data into DataFrame\n",
    "series_data = json_content['Results']['series'][0]['data']\n",
    "df = pd.DataFrame(series_data)\n",
    "df = df[[\"year\", \"period\", \"value\", \"footnotes\"]]\n",
    "print(\"DataFrame Shape: \", df.shape)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.415724,
   "end_time": "2025-08-18T22:53:04.796145",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T22:52:50.380421",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
