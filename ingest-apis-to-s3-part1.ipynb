{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b237ad12",
   "metadata": {
    "papermill": {
     "duration": 0.003043,
     "end_time": "2025-08-12T02:56:29.677529",
     "exception": false,
     "start_time": "2025-08-12T02:56:29.674486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# API Ingest → AWS S3 (BLS/DataUSA)\n",
    "This notebook automates the retrieval of BLS productivity data and stores it in Amazon S3.  \n",
    "It uses the BLS Public API with secure key management and includes duplicate protection to avoid re-uploading unchanged files.\n",
    "\n",
    "### What's Covered\n",
    "- **Automated sync** from data api source to S3.\n",
    "- **No hardcoded file names** – dynamically scrapes the BLS file list.\n",
    "- **403 error handling** – uses a valid User-Agent to comply with BLS access policy.\n",
    "- **Cloud-based execution** – runs in Kaggle with secure secret management.\n",
    "- **Secrets used** – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY.\n",
    "- **Duplicate protection** – checks content hashes before uploading.\n",
    "- **Optional scheduling** – can be run daily or weekly with no manual effort.\n",
    "\n",
    "### How It Works\n",
    "1. Fetch the current list of files from the BLS public directory.\n",
    "2. Download each file and compare its hash to the version in S3.\n",
    "3. Upload new or changed files to the configured S3 bucket.\n",
    "4. Skip unchanged files to save bandwidth and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d91c3d",
   "metadata": {
    "papermill": {
     "duration": 0.002341,
     "end_time": "2025-08-12T02:56:29.683067",
     "exception": false,
     "start_time": "2025-08-12T02:56:29.680726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Connect to AWS S3\n",
    "This notebook requires the following Python packages:  \n",
    "- boto3  \n",
    "- requests  \n",
    "- hashlib  \n",
    "- kaggle_secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION, BUCKET_NAME, BLS_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed12677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T02:56:29.689301Z",
     "iopub.status.busy": "2025-08-12T02:56:29.688969Z",
     "iopub.status.idle": "2025-08-12T02:56:31.836195Z",
     "shell.execute_reply": "2025-08-12T02:56:31.835243Z"
    },
    "papermill": {
     "duration": 2.152525,
     "end_time": "2025-08-12T02:56:31.838007",
     "exception": false,
     "start_time": "2025-08-12T02:56:29.685482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  6\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d153cd85",
   "metadata": {
    "papermill": {
     "duration": 0.002631,
     "end_time": "2025-08-12T02:56:31.845058",
     "exception": false,
     "start_time": "2025-08-12T02:56:31.842427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch BLS Data via API Key\n",
    "- Authenticate with a registered BLS API key to comply with access policies.  \n",
    "- Retrieve U.S. inflation data programmatically through the BLS Public API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfbc784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T02:56:31.852381Z",
     "iopub.status.busy": "2025-08-12T02:56:31.851241Z",
     "iopub.status.idle": "2025-08-12T02:56:32.819496Z",
     "shell.execute_reply": "2025-08-12T02:56:32.818348Z"
    },
    "papermill": {
     "duration": 0.973471,
     "end_time": "2025-08-12T02:56:32.821245",
     "exception": false,
     "start_time": "2025-08-12T02:56:31.847774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded bls_data.json to S3.\n"
     ]
    }
   ],
   "source": [
    "# API payload\n",
    "headers = {'Content-type': 'application/json'}\n",
    "data = json.dumps({\n",
    "    \"seriesid\": [\"CUUR0000SA0\", \"SUUR0000SA0\"],  # You can customize this\n",
    "    \"startyear\": \"2020\",\n",
    "    \"endyear\": \"2024\",\n",
    "    \"registrationkey\": API_KEY\n",
    "})\n",
    "\n",
    "# Make request\n",
    "response = requests.post(\n",
    "    \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "    data=data,\n",
    "    headers=headers\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    \n",
    "    # Save locally\n",
    "    with open(\"bls_data.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Upload JSON file to S3 bucket\n",
    "    s3.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=\"bls_data.json\",\n",
    "        Body=json.dumps(results, indent=2)\n",
    "    )\n",
    "    \n",
    "    print(\"Uploaded bls_data.json to S3.\")\n",
    "else:\n",
    "    print(\"Error: \", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb075f5",
   "metadata": {
    "papermill": {
     "duration": 0.002463,
     "end_time": "2025-08-12T02:56:32.826559",
     "exception": false,
     "start_time": "2025-08-12T02:56:32.824096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **API Data Pipeline – Send Files to S3 (No API Key Needed)**\n",
    "- **Custom User-Agent** to comply with BLS access rules and avoid 403 errors.  \n",
    "- **Checks for changes** by comparing file hashes before uploading.  \n",
    "- **Uploads only when updated**, reducing bandwidth usage and S3 storage costs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da908fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T02:56:32.833159Z",
     "iopub.status.busy": "2025-08-12T02:56:32.832428Z",
     "iopub.status.idle": "2025-08-12T02:56:35.075329Z",
     "shell.execute_reply": "2025-08-12T02:56:35.074309Z"
    },
    "papermill": {
     "duration": 2.24781,
     "end_time": "2025-08-12T02:56:35.076840",
     "exception": false,
     "start_time": "2025-08-12T02:56:32.829030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr.data.0.Current  File Downloaded from BLS\n",
      "No changes detected. Upload skipped.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import requests\n",
    "\n",
    "# CONFIG \n",
    "file_name = \"pr.data.0.Current\"\n",
    "bls_url = f\"https://download.bls.gov/pub/time.series/pr/{file_name}\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"ScottSchmidt/1.0 (scott.schmidt1989@yahoo.com)\"\n",
    "}\n",
    "\n",
    "# Download file from BLS:\n",
    "print(file_name, \" File Downloaded from BLS\")\n",
    "response = requests.get(bls_url, headers=headers)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to download BLS data:\", response.status_code)\n",
    "else:\n",
    "    content = response.text\n",
    "    content_hash = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    try:\n",
    "        existing_obj = s3.get_object(Bucket=BUCKET_NAME, Key=file_name)\n",
    "        existing_content = existing_obj['Body'].read().decode(\"utf-8\")\n",
    "        existing_hash = hashlib.md5(existing_content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if content_hash == existing_hash:\n",
    "            print(\"No changes detected. Upload skipped.\")\n",
    "        else:\n",
    "            s3.put_object(Bucket=BUCKET_NAME, Key=file_name, Body=content)\n",
    "            print(\"Updated file uploaded to S3.\")\n",
    "    except s3.exceptions.NoSuchKey:\n",
    "        # File doesn't exist yet\n",
    "        s3.put_object(Bucket=BUCKET_NAME, Key=file_name, Body=content)\n",
    "        print(\"File not found in S3. Uploaded new file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90130ae0",
   "metadata": {
    "papermill": {
     "duration": 0.002783,
     "end_time": "2025-08-12T02:56:35.082863",
     "exception": false,
     "start_time": "2025-08-12T02:56:35.080080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fetch and Upload BLS Data for Each Series ID\n",
    "This section goes through each BLS series ID, requests data from the API, \n",
    "and uploads it to S3 only if the data has changed since the last upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6f525b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T02:56:35.090131Z",
     "iopub.status.busy": "2025-08-12T02:56:35.089413Z",
     "iopub.status.idle": "2025-08-12T02:56:37.050506Z",
     "shell.execute_reply": "2025-08-12T02:56:37.049490Z"
    },
    "papermill": {
     "duration": 1.966205,
     "end_time": "2025-08-12T02:56:37.052020",
     "exception": false,
     "start_time": "2025-08-12T02:56:35.085815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching CUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  CUUR0000SA0.json\n",
      "Fetching SUUR0000SA0 from BLS API...\n",
      "Uploaded to S3!  SUUR0000SA0.json\n"
     ]
    }
   ],
   "source": [
    "# Series IDs you want to fetch\n",
    "series_ids = [\"CUUR0000SA0\", \"SUUR0000SA0\"]  \n",
    "\n",
    "# Loop through each series and fetch data from the BLS API\n",
    "for series_id in series_ids:\n",
    "    print(f\"Fetching {series_id} from BLS API...\")\n",
    "\n",
    "    # Prepare API request payload\n",
    "    payload = json.dumps({\n",
    "        \"seriesid\": [series_id],\n",
    "        \"startyear\": \"2020\",\n",
    "        \"endyear\": \"2024\",\n",
    "        \"registrationkey\": API_KEY\n",
    "    })\n",
    "\n",
    "    # Send POST request to the BLS API\n",
    "    response = requests.post(\n",
    "        \"https://api.bls.gov/publicAPI/v2/timeseries/data/\",\n",
    "        data=payload,\n",
    "        headers={\"Content-type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    #  Skip if API request fails\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {series_id}: {response.status_code}\")\n",
    "        continue\n",
    "\n",
    "    # Skip if API request fails\n",
    "    content = response.text\n",
    "    hash_new = hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "    s3_key = f\"{series_id}.json\"\n",
    "\n",
    "    # Check if file already exists in S3 with same content\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "        existing = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "        hash_existing = hashlib.md5(existing.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        if hash_existing == hash_new:\n",
    "            print(series_id, \" Skipping. Unchanged series_id\")\n",
    "            continue\n",
    "    except ClientError:\n",
    "        pass  # File doesn't exist \n",
    "\n",
    "    # Upload to S3\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Key=s3_key, Body=content)\n",
    "    print(\"Uploaded to S3! \", s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864fba2",
   "metadata": {
    "papermill": {
     "duration": 0.002893,
     "end_time": "2025-08-12T02:56:37.058139",
     "exception": false,
     "start_time": "2025-08-12T02:56:37.055246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preview BLS Data from S3\n",
    "This section retrieves a specific BLS JSON file from Amazon S3, \n",
    "converts it into a Pandas DataFrame, reorders the columns, \n",
    "and displays the first few rows for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83dfa550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-12T02:56:37.066019Z",
     "iopub.status.busy": "2025-08-12T02:56:37.065202Z",
     "iopub.status.idle": "2025-08-12T02:56:39.187279Z",
     "shell.execute_reply": "2025-08-12T02:56:39.186225Z"
    },
    "papermill": {
     "duration": 2.127823,
     "end_time": "2025-08-12T02:56:39.189087",
     "exception": false,
     "start_time": "2025-08-12T02:56:37.061264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  (60, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "      <th>footnotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024</td>\n",
       "      <td>M12</td>\n",
       "      <td>315.605</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>M11</td>\n",
       "      <td>315.493</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>M10</td>\n",
       "      <td>315.664</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>M09</td>\n",
       "      <td>315.301</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024</td>\n",
       "      <td>M08</td>\n",
       "      <td>314.796</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024</td>\n",
       "      <td>M07</td>\n",
       "      <td>314.540</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024</td>\n",
       "      <td>M06</td>\n",
       "      <td>314.175</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024</td>\n",
       "      <td>M05</td>\n",
       "      <td>314.069</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024</td>\n",
       "      <td>M04</td>\n",
       "      <td>313.548</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024</td>\n",
       "      <td>M03</td>\n",
       "      <td>312.332</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024</td>\n",
       "      <td>M02</td>\n",
       "      <td>310.326</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024</td>\n",
       "      <td>M01</td>\n",
       "      <td>308.417</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023</td>\n",
       "      <td>M12</td>\n",
       "      <td>306.746</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023</td>\n",
       "      <td>M11</td>\n",
       "      <td>307.051</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023</td>\n",
       "      <td>M10</td>\n",
       "      <td>307.671</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023</td>\n",
       "      <td>M09</td>\n",
       "      <td>307.789</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023</td>\n",
       "      <td>M08</td>\n",
       "      <td>307.026</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>M07</td>\n",
       "      <td>305.691</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023</td>\n",
       "      <td>M06</td>\n",
       "      <td>305.109</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023</td>\n",
       "      <td>M05</td>\n",
       "      <td>304.127</td>\n",
       "      <td>[{}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year period    value footnotes\n",
       "0   2024    M12  315.605      [{}]\n",
       "1   2024    M11  315.493      [{}]\n",
       "2   2024    M10  315.664      [{}]\n",
       "3   2024    M09  315.301      [{}]\n",
       "4   2024    M08  314.796      [{}]\n",
       "5   2024    M07  314.540      [{}]\n",
       "6   2024    M06  314.175      [{}]\n",
       "7   2024    M05  314.069      [{}]\n",
       "8   2024    M04  313.548      [{}]\n",
       "9   2024    M03  312.332      [{}]\n",
       "10  2024    M02  310.326      [{}]\n",
       "11  2024    M01  308.417      [{}]\n",
       "12  2023    M12  306.746      [{}]\n",
       "13  2023    M11  307.051      [{}]\n",
       "14  2023    M10  307.671      [{}]\n",
       "15  2023    M09  307.789      [{}]\n",
       "16  2023    M08  307.026      [{}]\n",
       "17  2023    M07  305.691      [{}]\n",
       "18  2023    M06  305.109      [{}]\n",
       "19  2023    M05  304.127      [{}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file content \n",
    "key = \"CUUR0000SA0.json\" # SUUR0000SA0\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=key)\n",
    "json_content = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "\n",
    "# Extract data into DataFrame\n",
    "series_data = json_content['Results']['series'][0]['data']\n",
    "df = pd.DataFrame(series_data)\n",
    "df = df[[\"year\", \"period\", \"value\", \"footnotes\"]]\n",
    "print(\"DataFrame Shape: \", df.shape)\n",
    "df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.039319,
   "end_time": "2025-08-12T02:56:39.814035",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-12T02:56:24.774716",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
