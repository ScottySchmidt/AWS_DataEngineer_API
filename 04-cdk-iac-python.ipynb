{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":8.552609,"end_time":"2025-08-13T15:32:06.256238","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-13T15:31:57.703629","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"99920abe","cell_type":"markdown","source":"# # AWS CDK Deployment – Part 4 - Python Version (no cloudshell)\nThis step defines the infrastructure for the data pipeline using AWS CDK (Python) to stand up and wire everything for automated ingestion and processing of the BLS + population datasets.\n\n**[View CloudShell/CDK Logs — Part 4 (Sanitized)](https://github.com/ScottySchmidt/AWS_DataEngineer_API/tree/main/docs/part4)**  \n↑ That link shows the CloudShell based deploy for reference; this section documents the pure Python/CDK approach.\n\nUses AWS CDK (Python) to deploy the pipeline infrastructure:\n- Lambda, S3, and SQS resources\n- A daily ingest job (Parts 1 & 2)\n- Automatic processing (Part 3) when new files land in S3\n\n### Pipeline Flow\n1. Deploy infrastructure with CDK (S3, Lambdas, SQS, EventBridge, IAM).\n2. EventBridge triggers the Ingest Lambda daily.\n3. Ingest writes fresh datasets to S3.\n4. S3 emits object-created events to SQS (filtered for your files).\n5. SQS invokes the Report Lambda to process/aggregate and write outputs.\n6. CloudWatch Logs capture logs/metrics for both Lambdas.","metadata":{"papermill":{"duration":0.002601,"end_time":"2025-08-13T15:32:02.735622","exception":false,"start_time":"2025-08-13T15:32:02.733021","status":"completed"},"tags":[]}},{"id":"99e6bd5d","cell_type":"code","source":"import os, json, boto3\nimport requests\nimport hashlib\nfrom kaggle_secrets import UserSecretsClient\nimport time \nstart = time.time()\nprint(\"Running..\")\n\n# Load AWS secrets\nsecrets = UserSecretsClient()\nAPI_KEY = secrets.get_secret(\"BLS_API_KEY\")\nAWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\nAWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\nAWS_REGION = secrets.get_secret(\"AWS_REGION\")\nBUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n\n# Setup AWS session and S3\nsession = boto3.Session(\n    aws_access_key_id=AWS_ACCESS_KEY_ID,\n    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n    region_name=AWS_REGION\n)\ns3 = session.client(\"s3\")\n\n# Test S3 connection WITHOUT revealing keys\ntry:\n    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n    num_files = response.get('KeyCount', 0)\n    print(\"S3 connection successful. Bucket contains: \", num_files)\nexcept Exception as e:\n    print(\"S3 connection failed: \", e)\nprint(\"Done.\")","metadata":{"execution":{"iopub.status.busy":"2025-08-13T19:45:00.993873Z","iopub.execute_input":"2025-08-13T19:45:00.994218Z"},"papermill":{"duration":1.694896,"end_time":"2025-08-13T15:32:04.432860","exception":false,"start_time":"2025-08-13T15:32:02.737964","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"094873fc","cell_type":"code","source":"REGION = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\nsqs = boto3.client(\"sqs\", region_name=REGION)\n\nresp = sqs.create_queue(\n    QueueName=\"JsonWriteQueue\",\n    Attributes={\"VisibilityTimeout\": \"300\"}  # seconds\n)\nQUEUE_URL = resp[\"QueueUrl\"]\nQUEUE_ATTR = sqs.get_queue_attributes(QueueUrl=QUEUE_URL, AttributeNames=[\"QueueArn\"])\nQUEUE_ARN = QUEUE_ATTR[\"Attributes\"][\"QueueArn\"]\n\nprint(\"SQS ready\")\nprint(\"ARN ready\")","metadata":{"papermill":{"duration":0.426927,"end_time":"2025-08-13T15:32:05.318927","exception":false,"start_time":"2025-08-13T15:32:04.892000","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"dfd2684f","cell_type":"code","source":"#LAMBDA SMOKE TEST: \nlam = session.client(\"lambda\")\nfns = lam.list_functions()['Functions']\nnames = []\nfor function in fns:\n    name = function[\"FunctionName\"]\n    names.append(name)\nprint(\"Functions:\", names)","metadata":{"papermill":{"duration":0.304375,"end_time":"2025-08-13T15:32:05.625784","exception":false,"start_time":"2025-08-13T15:32:05.321409","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e9639434-4998-4569-9b62-89b7724b4d6e","cell_type":"code","source":"from botocore.config import Config\n\n# CONFIG:\nAWS_REGION = os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\nINGEST_LAMBDA = os.getenv(\"INGEST_LAMBDA\", \"RearcBLSLambda\")  # your new ingest function\n\n# boto3 client with sensible timeouts/retries\nlam = boto3.client(\n    \"lambda\",\n    region_name=AWS_REGION,\n    config=Config(connect_timeout=3, read_timeout=10, retries={\"max_attempts\": 2})\n)\n\n# INVOKE:\nresp = lam.invoke(\n    FunctionName=INGEST_LAMBDA,\n    InvocationType=\"RequestResponse\",\n    Payload=json.dumps({}).encode(\"utf-8\")  # send an empty JSON payload\n)\n\nstatus = resp.get(\"StatusCode\")\nprint(\"Status:\", status)\n\n# Show function errors if any\nif resp.get(\"FunctionError\"):\n    print(\"FunctionError:\", resp[\"FunctionError\"])\n\n# Try to pretty-print the response payload\nraw = resp[\"Payload\"].read()\nif raw:\n    try:\n        payload = json.loads(raw)\n        # If Lambda returned {\"statusCode\":..., \"body\": \"<json string>\"}, decode body too\n        if isinstance(payload.get(\"body\"), str):\n            try:\n                payload[\"body\"] = json.loads(payload[\"body\"])\n            except Exception:\n                pass\n        print(\"Payload:\", json.dumps(payload, indent=2)[:2000])\n    except Exception:\n        print(\"Payload (raw):\", raw[:2000])\n\n# Simple success/fail line\nif status==200:\n    print(\"Success\")\nelse:\n    print(\"Issue\", status)\nprint(\"Done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"29261529-28f0-45a8-9fc4-7f34854a64b8","cell_type":"code","source":"import json, base64\nresp = lam.invoke(\n    FunctionName=INGEST_LAMBDA,  # your Lambda name\n    InvocationType=\"RequestResponse\"\n)\nstatus = resp.get(\"StatusCode\")\nerr = resp.get(\"FunctionError\")\n\n# Read the payload ONCE\nraw_payload = resp[\"Payload\"].read()\n\n# Decode Lambda logs if they exist\nif \"LogResult\" in resp:\n    print(\"Lambda Logs\")\n    print(base64.b64decode(resp[\"LogResult\"]).decode(\"utf-8\", \"ignore\"))\n\n# Decode \nif raw_payload:\n    try:\n        payload_json = json.loads(raw_payload.decode(\"utf-8\"))\n        print(\"Payload\")\n        print(json.dumps(payload_json, indent=2))\n    except Exception:\n        print(\"Payload (raw)\")\n        print(raw_payload.decode(\"utf-8\", \"ignore\"))\nelse:\n    print(\"Payload is empty\")\n\nif status == 200:\n    print(\"Result. Status = 200\")\nelse:\n    print(\"Error: \", err)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2c1bb84b-4cb9-4520-8a49-4b02cb3b17af","cell_type":"code","source":"end = time.time()\nprint(round(end - start, 2), \"seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"77aab320","cell_type":"markdown","source":"### Conclusion\nThat wraps up Part 4 – Infrastructure as Code & the automated pipeline.\n\n[View GitHub Repository with All Parts](https://github.com/ScottySchmidt/AWS_DataEngineer_API)","metadata":{"papermill":{"duration":0.00181,"end_time":"2025-08-13T15:32:05.633848","exception":false,"start_time":"2025-08-13T15:32:05.632038","status":"completed"},"tags":[]}}]}