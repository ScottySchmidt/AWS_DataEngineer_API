{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "403713fb",
   "metadata": {
    "papermill": {
     "duration": 0.002751,
     "end_time": "2025-08-13T16:15:08.060844",
     "exception": false,
     "start_time": "2025-08-13T16:15:08.058093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# # AWS CDK Deployment – Part 4 - Python Version (no cloudshell)\n",
    "This step defines the infrastructure for the data pipeline using AWS CDK (Python) to stand up and wire everything for automated ingestion and processing of the BLS + population datasets.\n",
    "\n",
    "**[View CloudShell/CDK Logs — Part 4 (Sanitized)](https://github.com/ScottySchmidt/AWS_DataEngineer_API/tree/main/docs/part4)**  \n",
    "↑ That link shows the CloudShell based deploy for reference; this section documents the pure Python/CDK approach.\n",
    "\n",
    "Uses AWS CDK (Python) to deploy the pipeline infrastructure:\n",
    "- Lambda, S3, and SQS resources\n",
    "- A daily ingest job (Parts 1 & 2)\n",
    "- Automatic processing (Part 3) when new files land in S3\n",
    "\n",
    "### Pipeline Flow\n",
    "1. Deploy infrastructure with CDK (S3, Lambdas, SQS, EventBridge, IAM).\n",
    "2. EventBridge triggers the Ingest Lambda daily.\n",
    "3. Ingest writes fresh datasets to S3.\n",
    "4. S3 emits object-created events to SQS (filtered for your files).\n",
    "5. SQS invokes the Report Lambda to process/aggregate and write outputs.\n",
    "6. CloudWatch Logs capture logs/metrics for both Lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac006d4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T16:15:08.066401Z",
     "iopub.status.busy": "2025-08-13T16:15:08.066070Z",
     "iopub.status.idle": "2025-08-13T16:15:09.365118Z",
     "shell.execute_reply": "2025-08-13T16:15:09.363724Z"
    },
    "papermill": {
     "duration": 1.303849,
     "end_time": "2025-08-13T16:15:09.366820",
     "exception": false,
     "start_time": "2025-08-13T16:15:08.062971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 connection successful. Bucket contains:  6\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Load AWS secrets\n",
    "secrets = UserSecretsClient()\n",
    "API_KEY = secrets.get_secret(\"BLS_API_KEY\")\n",
    "AWS_ACCESS_KEY_ID = secrets.get_secret(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = secrets.get_secret(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_REGION = secrets.get_secret(\"AWS_REGION\")\n",
    "BUCKET_NAME = secrets.get_secret(\"BUCKET_NAME\")\n",
    "\n",
    "# Setup AWS session and S3\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Test connection WITHOUT revealing keys\n",
    "try:\n",
    "    response = s3.list_objects_v2(Bucket=BUCKET_NAME)\n",
    "    num_files = response.get('KeyCount', 0)\n",
    "    print(\"S3 connection successful. Bucket contains: \", num_files)\n",
    "except Exception as e:\n",
    "    print(\"S3 connection failed: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1de02d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T16:15:09.372304Z",
     "iopub.status.busy": "2025-08-13T16:15:09.371823Z",
     "iopub.status.idle": "2025-08-13T16:15:09.694712Z",
     "shell.execute_reply": "2025-08-13T16:15:09.693575Z"
    },
    "papermill": {
     "duration": 0.327562,
     "end_time": "2025-08-13T16:15:09.696482",
     "exception": false,
     "start_time": "2025-08-13T16:15:09.368920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Kaggle secrets to env vars.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# Pull from Kaggle Secrets and mirror into environment vars\n",
    "for key in [\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_REGION\", \"BUCKET_NAME\"]:\n",
    "    v = secrets.get_secret(key)\n",
    "    if v:\n",
    "        os.environ[key] = v\n",
    "\n",
    "# If only AWS_REGION is set, mirror it to AWS_DEFAULT_REGION for libraries that expect that\n",
    "if os.environ.get(\"AWS_REGION\") and not os.environ.get(\"AWS_DEFAULT_REGION\"):\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = os.environ[\"AWS_REGION\"]\n",
    "\n",
    "print(\"Exported Kaggle secrets to env vars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d1664d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T16:15:09.702368Z",
     "iopub.status.busy": "2025-08-13T16:15:09.702085Z",
     "iopub.status.idle": "2025-08-13T16:15:10.117345Z",
     "shell.execute_reply": "2025-08-13T16:15:10.116085Z"
    },
    "papermill": {
     "duration": 0.420135,
     "end_time": "2025-08-13T16:15:10.119153",
     "exception": false,
     "start_time": "2025-08-13T16:15:09.699018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQS ready\n",
      "ARN ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "REGION = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "sqs = boto3.client(\"sqs\", region_name=REGION)\n",
    "\n",
    "resp = sqs.create_queue(\n",
    "    QueueName=\"JsonWriteQueue\",\n",
    "    Attributes={\"VisibilityTimeout\": \"300\"}  # seconds\n",
    ")\n",
    "QUEUE_URL = resp[\"QueueUrl\"]\n",
    "QUEUE_ATTR = sqs.get_queue_attributes(QueueUrl=QUEUE_URL, AttributeNames=[\"QueueArn\"])\n",
    "QUEUE_ARN = QUEUE_ATTR[\"Attributes\"][\"QueueArn\"]\n",
    "\n",
    "print(\"SQS ready\")\n",
    "print(\"ARN ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405599b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T16:15:10.125559Z",
     "iopub.status.busy": "2025-08-13T16:15:10.125211Z",
     "iopub.status.idle": "2025-08-13T16:15:10.370183Z",
     "shell.execute_reply": "2025-08-13T16:15:10.369158Z"
    },
    "papermill": {
     "duration": 0.25018,
     "end_time": "2025-08-13T16:15:10.371936",
     "exception": false,
     "start_time": "2025-08-13T16:15:10.121756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions: ['BlsPipelineStack-BlsReportFn1ABE5E21-0qZIzspWVVtL', 'CFN-SM-IM-Lambda-catalog-DelayLambda-GH20qvggV0UO', 'BlsPipelineStack-BlsIngestFnCD0BBA37-KqHbWeaT1WT3', 'bls-ingest', 'CFGetCatalogRoles', 'RearcBLSLambda', 'CFGetDefaultVpcIdTut', 'MyFirstLambda', 'S3FileProcessorLambda', 'GitHubWebhookLambda', 'BlsPipelineStack-BucketNotificationsHandler050a058-iFZUwiLm9Toh', 'CFEnableSagemakerProjectsTut']\n"
     ]
    }
   ],
   "source": [
    "#LAMBDA SMOKE TEST: \n",
    "lam = session.client(\"lambda\")\n",
    "fns = lam.list_functions()['Functions']\n",
    "names = []\n",
    "for function in fns:\n",
    "    name = function[\"FunctionName\"]\n",
    "    names.append(name)\n",
    "print(\"Functions:\", names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68901cf9",
   "metadata": {
    "papermill": {
     "duration": 0.001936,
     "end_time": "2025-08-13T16:15:10.376303",
     "exception": false,
     "start_time": "2025-08-13T16:15:10.374367",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0312fc98",
   "metadata": {
    "papermill": {
     "duration": 0.001787,
     "end_time": "2025-08-13T16:15:10.380203",
     "exception": false,
     "start_time": "2025-08-13T16:15:10.378416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "That wraps up Part 4 – Infrastructure as Code & the automated pipeline.\n",
    "\n",
    "Here’s what we confirmed:\n",
    "The stack deployed successfully with no errors.\n",
    "The ingestion Lambda (combining Parts 1 & 2) ran and pulled the data.\n",
    "New uploads to S3 triggered the SQS queue as expected.\n",
    "The reporting Lambda processed those messages and logged the results.\n",
    "Final datasets are safely stored in S3 (see screenshot above).\n",
    "\n",
    "[View GitHub Repository with All Parts](https://github.com/ScottySchmidt/AWS_DataEngineer_API)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.375573,
   "end_time": "2025-08-13T16:15:11.002882",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-13T16:15:03.627309",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
